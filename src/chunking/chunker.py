# intelligent_chunker.py

import json
import argparse
import os
import re
import uuid
from typing import List, Dict, Any
import tiktoken
import yaml
from langchain.text_splitter import RecursiveCharacterTextSplitter

class SimpleChunker:
    """
    Crawler'dan gelen metni RAG iÃ§in temel parÃ§alara ayÄ±ran basit sistem.
    - KarmaÅŸÄ±k Parent Document, iÃ§erik tespiti ve dinamik overlap kaldÄ±rÄ±ldÄ±.
    - Crawler'Ä±n yeni Ã§Ä±ktÄ±sÄ±yla (--- PAGE BREAK ---) uyumlu hale getirildi.
    """
    
    def __init__(self, chunk_size: int = 400, chunk_overlap: int = 50, model_name: str = "gpt-3.5-turbo"):
        """
        Chunker'Ä± temel parametrelerle baÅŸlatÄ±r.
        
        Args:
            chunk_size (int): Her bir parÃ§anÄ±n hedef token boyutu.
            chunk_overlap (int): ParÃ§alar arasÄ±ndaki Ã¶rtÃ¼ÅŸme (token cinsinden).
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # Token sayacÄ± (encoder)
        try:
            self.encoder = tiktoken.encoding_for_model(model_name)
            print(f"âœ… Model iÃ§in token sayacÄ± yÃ¼klendi: {model_name}")
        except KeyError:
            self.encoder = tiktoken.get_encoding("cl100k_base")
            print(f"âš ï¸ UyarÄ±: Model bulunamadÄ±, varsayÄ±lan encoder kullanÄ±lÄ±yor.")
            
        print(f"âœ… SimpleChunker baÅŸlatÄ±ldÄ±. Hedef boyut: {chunk_size} token, Ã–rtÃ¼ÅŸme: {chunk_overlap} token.")

    def count_tokens(self, text: str) -> int:
        """Verilen metnin token sayÄ±sÄ±nÄ± dÃ¶ndÃ¼rÃ¼r."""
        return len(self.encoder.encode(text))

    def parse_pages_from_txt(self, txt_file_path: str) -> List[Dict[str, str]]:
        """
        SadeleÅŸtirilmiÅŸ crawler'Ä±n YENÄ° Ã§Ä±ktÄ±sÄ±nÄ± (URL ve TITLE iÃ§eren) okur.
        """
        try:
            with open(txt_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except FileNotFoundError:
            print(f"âŒ Hata: GiriÅŸ dosyasÄ± bulunamadÄ± -> {txt_file_path}")
            return []

        pages = []
        raw_pages = content.split('--- PAGE BREAK ---')
        
        for raw_page in raw_pages:
            raw_page = raw_page.strip()
            if not raw_page:
                continue
            
            lines = raw_page.split('\n')
            if len(lines) < 2:  # En azÄ±ndan URL, TITLE ve iÃ§erik olmalÄ±
                continue
            
            url_line = lines[0]
            title_line = lines[1]
            
            url_match = re.match(r"URL: (https?://[^\s]+)", url_line)
            title_match = re.match(r"TITLE: (.+)", title_line)

            if url_match and title_match:
                url = url_match.group(1).strip()
                title = title_match.group(1).strip()
                # Geriye kalan her ÅŸey iÃ§eriktir
                page_content = '\n'.join(lines[2:]).strip()
                pages.append({"url": url, "title": title, "content": page_content})
            
        
        print(f"ğŸ“„ {len(pages)} adet sayfa (baÅŸlÄ±k bilgisiyle) baÅŸarÄ±yla okundu.")
        return pages

    def create_chunks(self, pages: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """
        Okunan sayfalardan parÃ§acÄ±klar (chunk) oluÅŸturur.
        """
        all_chunks = []
        
        # LangChain'in hazÄ±r ve etkili text splitter'Ä±nÄ± kullanalÄ±m.
        # Bu, kendi 'split_text_smartly' fonksiyonumuzu yazmaktan daha basit ve standart bir yoldur.
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=self.count_tokens,
            separators=["\n\n", "\n", " ", ""] # BÃ¶lme Ã¶ncelik sÄ±rasÄ±
        )
        
        for page in pages:
            # Metni LangChain splitter ile bÃ¶l
            chunks_content = text_splitter.split_text(page['content'])
            
            for i, chunk_text in enumerate(chunks_content):
                chunk = {
                    "id": str(uuid.uuid4()), # Her chunk iÃ§in eÅŸsiz bir ID
                    "content": chunk_text,
                    "metadata": {
                        "source_url": page['url'],
                        "title": page['title'],
                        "chunk_index": i + 1,
                        "token_count": self.count_tokens(chunk_text)
                    }
                }
                all_chunks.append(chunk)
        
        print(f"ğŸ‰ Toplam {len(all_chunks)} adet chunk oluÅŸturuldu.")
        return all_chunks

    def save_chunks_to_json(self, chunks: List[Dict[str, Any]], output_file: str):
        """OluÅŸturulan chunk'larÄ± basit bir JSON dosyasÄ±na kaydeder."""
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(chunks, f, ensure_ascii=False, indent=2)
            
        print(f"ğŸ’¾ Chunk'lar baÅŸarÄ±yla '{output_file}' dosyasÄ±na kaydedildi.")

def load_config(config_path: str = 'config/config.yaml') -> Dict:
    """YAML config dosyasÄ±nÄ± yÃ¼kler."""
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            try:
                print(f"ğŸ“‹ Ayarlar ÅŸuradan yÃ¼kleniyor: {config_path}")
                return yaml.safe_load(f) or {}
            except yaml.YAMLError as e:
                print(f"âŒ Config dosyasÄ± okunurken hata oluÅŸtu: {e}")
    return {}

def main():
    parser = argparse.ArgumentParser(description="Metin dosyasÄ±nÄ± RAG iÃ§in parÃ§alara (chunk) bÃ¶ler.")
    parser.add_argument("input_file", help="Crawler tarafÄ±ndan oluÅŸturulan girdi .txt dosyasÄ±.")
    parser.add_argument("--output", "-o", default="data/chunks/chunks.json", help="Ã‡Ä±ktÄ± JSON dosyasÄ±nÄ±n yolu.")
    parser.add_argument("--chunk-size", "-s", type=int, default=400, help="Hedef chunk boyutu (token).")
    parser.add_argument("--chunk-overlap", "-v", type=int, default=50, help="Chunk'lar arasÄ± Ã¶rtÃ¼ÅŸme (token).")

    args = parser.parse_args()

    # --- CONFIG ---
    config = load_config()
    chunking_config = config.get('chunking', {})

    # DeÄŸerleri belirle (Komut satÄ±rÄ± > config dosyasÄ± > varsayÄ±lan)
    input_file = args.input_file or chunking_config.get('input_file', 'data/raw/jotform_trial.txt')
    output_file = args.output or chunking_config.get('output_file', 'data/chunks/chunks.json')
    chunk_size = args.chunk_size or chunking_config.get('target_chunk_size', 400)
    chunk_overlap = args.chunk_overlap or chunking_config.get('overlap_size', 50)
    # --- CONFIG ---


    # Chunker'Ä± baÅŸlat
    chunker = SimpleChunker(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )

    # Crawler Ã§Ä±ktÄ±sÄ±nÄ± oku ve sayfalara ayÄ±r
    pages = chunker.parse_pages_from_txt(input_file)

    if pages:
        chunks = chunker.create_chunks(pages)
        if chunks:
            chunker.save_chunks_to_json(chunks, output_file)


if __name__ == "__main__":
    main()