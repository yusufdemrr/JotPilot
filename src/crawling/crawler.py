# crawler.py

import asyncio
import os
import argparse
import re
import yaml
from urllib.parse import urljoin, urlparse
from crawl4ai import AsyncWebCrawler
from bs4 import BeautifulSoup

def clean_text_content(text: str) -> str:
    """
    Metin iÃ§eriÄŸini RAG iÃ§in temizler.
    - Gereksiz boÅŸluklarÄ± ve satÄ±r sonlarÄ±nÄ± tek boÅŸluÄŸa indirir.
    - Tekrar eden veya anlamsÄ±z kÄ±sa cÃ¼mleleri kaldÄ±rÄ±r.
    """
    # 1. AdÄ±m: TÃ¼m Ã§oklu boÅŸluklarÄ±, tab'larÄ± ve yeni satÄ±rlarÄ± tek boÅŸluÄŸa indirge
    cleaned_text = re.sub(r'\s+', ' ', text).strip()
    
    # 2. AdÄ±m: Metni cÃ¼mlelere bÃ¶l
    # Nokta, soru iÅŸareti, Ã¼nlemden sonra boÅŸluk olan yerlerden bÃ¶ler
    sentences = re.split(r'(?<=[.?!])\s+', cleaned_text)
    
    unique_sentences = []
    seen_phrases = set() # GÃ¶rÃ¼len cÃ¼mle baÅŸlÄ±klarÄ±nÄ± saklamak iÃ§in
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue

        # Ã‡ok kÄ±sa veya anlamsÄ±z cÃ¼mleleri atla (genellikle menÃ¼ Ã¶ÄŸeleri)
        if len(sentence.split()) < 4:
            continue
            
        # CÃ¼mlenin ilk 50 karakterini (harf ve rakamlar) anahtar olarak kullan
        # Bu, "Click here for more." ve "Click here for more information." gibi
        # Ã§ok benzer cÃ¼mleleri yakalamaya yardÄ±mcÄ± olur.
        sentence_key = re.sub(r'[^\w\s]', '', sentence.lower())[:50]
        
        if sentence_key not in seen_phrases:
            seen_phrases.add(sentence_key)
            unique_sentences.append(sentence)
            
    # TemizlenmiÅŸ ve eÅŸsiz cÃ¼mleleri yeniden birleÅŸtir
    return ' '.join(unique_sentences)

def simple_extract_content(html_content: str) -> str:
    """
    HTML'den ana iÃ§eriÄŸi basitÃ§e Ã§Ä±karan ve temizleyen fonksiyon.
    """
    soup = BeautifulSoup(html_content, 'html.parser')

    # 1. AdÄ±m: Sayfa baÅŸlÄ±ÄŸÄ±nÄ± (<h1>) bul
    title_tag = soup.find('h1')
    title = title_tag.get_text(strip=True) if title_tag else "BaÅŸlÄ±k BulunamadÄ±"
    
    selectors = ['main', 'article', '[role="main"]', '.content', '#content']
    main_content_tag = None
    for selector in selectors:
        main_content_tag = soup.select_one(selector)
        if main_content_tag:
            break
            
    if not main_content_tag:
        main_content_tag = soup.body
        if not main_content_tag:
            return {"title": title, "content": ""}
        
    # h1 etiketini iÃ§erikten Ã§Ä±karalÄ±m ki tekrar etmesin
    if title_tag and main_content_tag.find('h1'):
        title_tag.decompose()

    for tag in main_content_tag.select('nav, footer, script, style, aside, header'):
        tag.decompose()
        
    raw_text = main_content_tag.get_text(separator=' ', strip=True)
    
    # Ham metni, tekrarlarÄ± ve boÅŸluklarÄ± temizleyen fonksiyondan geÃ§ir
    cleaned_text = clean_text_content(raw_text)
    
    return {"title": title, "content": cleaned_text}

async def crawl_site(start_url: str, max_depth: int, max_links: int):
    """
    Belirtilen sitede gezinir, iÃ§eriÄŸi toplar ve yapÄ±landÄ±rÄ±lmÄ±ÅŸ metin dÃ¶ndÃ¼rÃ¼r.
    """
    print(f"Crawling baÅŸlÄ±yor. BaÅŸlangÄ±Ã§ URL: {start_url}, Derinlik: {max_depth}")
    
    all_pages_content = []
    urls_to_visit = {start_url}
    visited_urls = set()
    
    async with AsyncWebCrawler(headless=True, verbose=False) as crawler:
        for depth in range(max_depth + 1):
            if not urls_to_visit:
                break
                
            current_level_urls = list(urls_to_visit)[:max_links]
            urls_to_visit.clear()
            
            print(f"\n--- Seviye {depth} ---")
            print(f"Gezilecek {len(current_level_urls)} URL var.")
            
            for url in current_level_urls:
                if url in visited_urls:
                    continue
                
                print(f"-> Geziliyor: {url}")
                visited_urls.add(url)
                
                try:
                    result = await crawler.arun(url=url, wait_for="networkidle")
                    
                    if not result.success or not result.html:
                        print(f"  âŒ BaÅŸarÄ±sÄ±z: {result.error_message}")
                        continue

                    page_data = simple_extract_content(result.html)
                    content = page_data['content']
                    title = page_data['title']
                    
                    if len(content) < 200:
                        print(f"  âš ï¸ Ä°Ã§erik temizlendikten sonra Ã§ok kÄ±sa kaldÄ± ({len(content)} karakter), atlanÄ±yor.")
                        continue
                        
                    formatted_content = f"URL: {url}\nTITLE: {title}\n\n{content}"
                    
                    all_pages_content.append(formatted_content)
                    print(f"  âœ… BaÅŸarÄ±lÄ±: '{title}' baÅŸlÄ±klÄ± sayfa eklendi.")

                    
                    if depth < max_depth:
                        soup = BeautifulSoup(result.html, 'html.parser')
                        base_domain = urlparse(url).netloc
                        for a_tag in soup.find_all('a', href=True):
                            href = a_tag['href']
                            full_url = urljoin(url, href)
                            if urlparse(full_url).netloc == base_domain and '/help/' in full_url:
                                urls_to_visit.add(full_url)
                                
                except Exception as e:
                    print(f"  âŒ Hata oluÅŸtu: {e}")

    return all_pages_content

def load_config(config_path: str = 'config/config.yaml') -> dict:
    """YAML config dosyasÄ±nÄ± yÃ¼kler ve crawling ayarlarÄ±nÄ± dÃ¶ndÃ¼rÃ¼r."""
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            try:
                config = yaml.safe_load(f)
                # Sadece 'crawling' bÃ¶lÃ¼mÃ¼nÃ¼ dÃ¶ndÃ¼r, yoksa boÅŸ dict dÃ¶ndÃ¼r
                return config.get('crawling', {})
            except yaml.YAMLError as e:
                print(f"âŒ Config dosyasÄ± okunurken hata oluÅŸtu: {e}")
    return {}

async def main(base_url: str, max_depth: int, output_file: str, max_links: int):
    """Ana fonksiyon. Crawl iÅŸlemini baÅŸlatÄ±r ve sonucu dosyaya yazar."""
    
    # base_url artÄ±k parametre olarak geliyor.
    all_content = await crawl_site(base_url, max_depth, max_links)
    
    if not all_content:
        print("\nHiÃ§ iÃ§erik bulunamadÄ±. Ä°ÅŸlem sonlandÄ±rÄ±lÄ±yor.")
        return

    final_text = "\n\n--- PAGE BREAK ---\n\n".join(all_content)
    
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(final_text)
        
    print(f"\nğŸ‰ Crawling tamamlandÄ±!")
    print(f"ğŸ“ SonuÃ§lar '{output_file}' dosyasÄ±na kaydedildi.")
    print(f"ğŸ“„ Toplam {len(all_content)} sayfa iÅŸlendi.")

if __name__ == "__main__":
    # Bu blok tamamen gÃ¼ncellenmiÅŸtir
    parser = argparse.ArgumentParser(description="Jotform Help sayfasÄ±nÄ± gezer ve iÃ§eriÄŸi bir dosyaya kaydeder.")
    parser.add_argument("--depth", "-d", type=int, help="Gezinme derinliÄŸi.")
    parser.add_argument("--output", "-o", type=str, help="Ã‡Ä±ktÄ± dosyasÄ±nÄ±n yolu.")
    parser.add_argument("--max-links", "-m", type=int, help="Her seviyede gezilecek maksimum link sayÄ±sÄ±.")
    
    args = parser.parse_args()

    # --- CONFIG ---
    config = load_config()

    # DeÄŸerleri belirle (Ã–ncelik sÄ±rasÄ±: Komut satÄ±rÄ± > config dosyasÄ± > varsayÄ±lan deÄŸer)
    base_url = config.get('base_url', 'https://www.jotform.com/help/')
    max_depth = args.depth or config.get('max_depth', 5)
    max_links = args.max_links or config.get('max_links_per_level', 100)
    output_file = args.output or config.get('output_file', 'data/raw/jotform_help_content.txt')
    # --- CONFIG ---

    asyncio.run(main(
        base_url=base_url,
        max_depth=max_depth, 
        output_file=output_file, 
        max_links=max_links
    ))