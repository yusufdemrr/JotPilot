# src/llm/chatbot.py

import yaml
from typing import List, TypedDict
from sentence_transformers import SentenceTransformer
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langgraph.graph import StateGraph, END
from dotenv import load_dotenv

# Import our other modules
from src.llm.openai_client import OpenAIClient
from src.embedding.qdrant_manager import QdrantManager

dotenv_path = 'config/.env'
load_dotenv(dotenv_path=dotenv_path)

# --- Step 1: Define the Graph's "Memory" (State) ---
# The State is the data structure passed between nodes in the graph.
class GraphState(TypedDict):
    query: str                      # The user's latest question
    context: str                    # The context retrieved from Qdrant
    response: str                   # The latest response generated by the LLM
    chat_history: List[BaseMessage] # The entire conversation history (in LangChain format)

class LangGraphChatbot:
    """
    A scalable and modern chatbot that manages the entire RAG process using LangGraph.
    """
    def __init__(self, config_path: str = 'config/config.yaml'):
        print("ðŸ¤– Initializing LangGraph Chatbot...")
        
        # --- Initialize Components ---
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)

        self.openai_client = OpenAIClient(self.config['llm'])
        self.qdrant_manager = QdrantManager(self.config['qdrant'])
        
        model_name = self.config['models']['primary']['model_name']
        print(f"Loading embedding model '{model_name}'...")
        self.embedding_model = SentenceTransformer(model_name)
        
        try:
            system_prompt_path = self.config['llm']['prompts']['rag_system_prompt_path']
            rag_template_path = self.config['llm']['prompts']['rag_template_path']

            with open(system_prompt_path, 'r', encoding='utf-8') as f:
                self.system_prompt = f.read()

            with open(rag_template_path, 'r', encoding='utf-8') as f:
                self.rag_template = f.read()
        
        except FileNotFoundError as e:
            print(f"âŒ ERROR: Prompt file not found: {e}")
            raise
        except KeyError as e:
            print(f"âŒ ERROR: Prompt path key not found in config.yaml: {e}")
            raise
        
        # --- Step 2: Build the LangGraph Workflow ---
        workflow = StateGraph(GraphState)

        # Add the nodes to the graph
        workflow.add_node("retrieve_context", self.retrieve_context)
        workflow.add_node("generate_response", self.generate_response)

        # Set the entry point for the graph
        workflow.set_entry_point("retrieve_context")

        # Define the connections (edges) between the nodes
        workflow.add_edge("retrieve_context", "generate_response")
        workflow.add_edge("generate_response", END) # The 'generate_response' step is the end of the graph

        # Compile the prepared workflow into a runnable graph
        self.graph = workflow.compile()
        print("âœ… LangGraph Chatbot initialized successfully and is ready.")

    # --- Step 3: Define the Graph Nodes as Functions ---

    def retrieve_context(self, state: GraphState) -> dict:
        """
        Node 1: Takes the user's query, searches in Qdrant, and finds the context.
        """
        print("-> Node: retrieve_context executing...")
        query = state["query"]
        
        query_vector = self.embedding_model.encode(query).tolist()
        
        search_results = self.qdrant_manager.search(
            query_vector=query_vector,
            limit=self.config['llm']['rag']['search_limit']
        )
        
        if not search_results:
            context = "No specific information was found on this topic."
        else:
            context_parts = [result.payload['content'] for result in search_results]
            context = "\n\n---\n\n".join(context_parts)
            
        return {"context": context}

    def generate_response(self, state: GraphState) -> dict:
        """
        Node 2: Takes the history, context, and query, sends them to the LLM, and generates a response.
        """
        print("-> Node: generate_response executing...")
        query = state["query"]
        context = state["context"]
        chat_history = state["chat_history"]

        # Format to basic text
        history_str = "\n".join([f"{msg.type}: {msg.content}" for msg in chat_history])
        
        prompt_with_context = (
            f"Previous Conversation History:\n{history_str}\n\n"
            f"Current Question: {query}"
        )

        final_prompt = self.rag_template.format(
            context=context,
            question=prompt_with_context
        )

        response = self.openai_client.get_completion(
            system_prompt=self.system_prompt,
            user_prompt=final_prompt
        )
        
        return {"response": response}

    def invoke(self, query: str, chat_history: List[BaseMessage]) -> str:
        """
        Runs the graph and returns the final response.
        """
        inputs = {"query": query, "chat_history": chat_history}
        # Run the graph with this initial state
        final_state = self.graph.invoke(inputs)
        return final_state["response"]

def main():
    """Runs the chatbot in interactive mode."""
    chatbot = LangGraphChatbot()
    
    # Memory is maintained in the main loop
    chat_history = []
    max_history = chatbot.config.get('chat', {}).get('max_history', 5)

    print("\n--- Jotform AI Agent ---")
    print("Hello! What would you like to know about Jotform? (Type 'exit' to quit)")
    
    while True:
        user_query = input("\nðŸ‘¤ You: ")
        if user_query.lower() in ['exit', 'quit', 'q']:
            print("Goodbye!")
            break
            
        # Run the graph with the current history
        response = chatbot.invoke(user_query, chat_history)
        
        print(f"\nðŸ¤– Agent: {response}")
        
        # Update the memory
        chat_history.append(HumanMessage(content=user_query))
        chat_history.append(AIMessage(content=response))
        
        # Prevent the history from growing too long
        if len(chat_history) > max_history * 2:
            chat_history = chat_history[-(max_history * 2):]

if __name__ == "__main__":
    main()