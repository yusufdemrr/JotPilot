# src/llm/chatbot_langgraph.py

import yaml
import os
from typing import List, TypedDict
from sentence_transformers import SentenceTransformer
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langgraph.graph import StateGraph, END

# DiÄŸer modÃ¼llerimizi import edelim
from src.llm.openai_client import OpenAIClient
from src.embedding.qdrant_manager import QdrantManager
from dotenv import load_dotenv

dotenv_path = 'config/.env'
load_dotenv(dotenv_path=dotenv_path)

# --- AdÄ±m 1: GrafiÄŸin "HafÄ±zasÄ±nÄ±" (State) TanÄ±mla ---
# State, grafikteki adÄ±mlar (node'lar) arasÄ±nda taÅŸÄ±nan veri yapÄ±sÄ±dÄ±r.
class GraphState(TypedDict):
    query: str                      # KullanÄ±cÄ±nÄ±n son sorusu
    context: str                    # Qdrant'tan Ã§ekilen bilgi
    response: str                   # LLM tarafÄ±ndan Ã¼retilen son cevap
    chat_history: List[BaseMessage] # TÃ¼m konuÅŸma geÃ§miÅŸi (LangChain formatÄ±nda)


class LangGraphChatbot:
    """
    TÃ¼m RAG sÃ¼recini LangGraph ile yÃ¶neten, Ã¶lÃ§eklenebilir ve modern chatbot.
    """
    def __init__(self, config_path: str = 'config/config.yaml'):
        print("ðŸ¤– LangGraph Chatbot baÅŸlatÄ±lÄ±yor...")
        
        # --- BileÅŸenleri BaÅŸlat ---
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)

        self.openai_client = OpenAIClient(self.config['llm'])
        self.qdrant_manager = QdrantManager(self.config['qdrant'])
        
        model_name = self.config['models']['primary']['model_name']
        print(f"'{model_name}' embedding modeli yÃ¼kleniyor...")
        self.embedding_model = SentenceTransformer(model_name)
        
        # Load prompt templates from the file paths
        try:
            system_prompt_path = self.config['llm']['prompts']['rag_system_prompt_path']
            rag_template_path = self.config['llm']['prompts']['rag_template_path']

            with open(system_prompt_path, 'r', encoding='utf-8') as f:
                self.system_prompt = f.read()

            with open(rag_template_path, 'r', encoding='utf-8') as f:
                self.rag_template = f.read()
        
        except FileNotFoundError as e:
            print(f"âŒ ERROR: Prompt file not found: {e}")
            raise
        except KeyError as e:
            print(f"âŒ ERROR: Prompt path key not found in config.yaml: {e}")
            raise
        
        # --- AdÄ±m 2: LangGraph AkÄ±ÅŸ ÅžemasÄ±nÄ± OluÅŸtur ---
        workflow = StateGraph(GraphState)

        # GrafiÄŸe adÄ±mlarÄ± (node'larÄ±) ekle
        workflow.add_node("retrieve_context", self.retrieve_context)
        workflow.add_node("generate_response", self.generate_response)

        # GrafiÄŸin baÅŸlangÄ±Ã§ noktasÄ±nÄ± belirle
        workflow.set_entry_point("retrieve_context")

        # AdÄ±mlar arasÄ±ndaki baÄŸlantÄ±larÄ± (edge'leri) kur
        workflow.add_edge("retrieve_context", "generate_response")
        workflow.add_edge("generate_response", END) # 'generate_response' adÄ±mÄ± sondur

        # HazÄ±rlanan akÄ±ÅŸ ÅŸemasÄ±nÄ± derle ve Ã§alÄ±ÅŸtÄ±rÄ±labilir hale getir
        self.graph = workflow.compile()
        print("âœ… LangGraph Chatbot baÅŸarÄ±yla baÅŸlatÄ±ldÄ± ve Ã§alÄ±ÅŸmaya hazÄ±r.")

    # --- AdÄ±m 3: Grafik DÃ¼ÄŸÃ¼mlerini (Node) Fonksiyon Olarak TanÄ±mla ---

    def retrieve_context(self, state: GraphState) -> dict:
        """
        Node 1: KullanÄ±cÄ±nÄ±n sorusunu alÄ±r, Qdrant'ta arama yapar ve context'i bulur.
        """
        print("-> Node: retrieve_context Ã§alÄ±ÅŸÄ±yor...")
        query = state["query"]
        
        query_vector = self.embedding_model.encode(query).tolist()
        
        search_results = self.qdrant_manager.search(
            query_vector=query_vector,
            limit=self.config['llm']['rag']['search_limit']
        )
        
        if not search_results:
            context = "Bu konu hakkÄ±nda spesifik bir bilgi bulunamadÄ±."
        else:
            context_parts = [result.payload['content'] for result in search_results]
            context = "\n\n---\n\n".join(context_parts)
            
        return {"context": context}

    def generate_response(self, state: GraphState) -> dict:
        """
        Node 2: GeÃ§miÅŸi, context'i ve soruyu alÄ±p LLM'e gÃ¶nderir, cevabÄ± Ã¼retir.
        """
        print("-> Node: generate_response Ã§alÄ±ÅŸÄ±yor...")
        query = state["query"]
        context = state["context"]
        chat_history = state["chat_history"]

        # KonuÅŸma geÃ§miÅŸini basit bir metne dÃ¶nÃ¼ÅŸtÃ¼r
        history_str = "\n".join([f"{msg.type}: {msg.content}" for msg in chat_history])
        
        prompt_with_context = (
            f"Ã–nceki KonuÅŸma GeÃ§miÅŸi:\n{history_str}\n\n"
            f"Åžimdiki Soru: {query}"
        )

        final_prompt = self.rag_template.format(
            context=context,
            question=prompt_with_context
        )

        response = self.openai_client.get_completion(
            system_prompt=self.system_prompt,
            user_prompt=final_prompt
        )
        
        return {"response": response}

    def invoke(self, query: str, chat_history: List[BaseMessage]) -> str:
        """
        GrafiÄŸi Ã§alÄ±ÅŸtÄ±rÄ±r ve nihai cevabÄ± dÃ¶ndÃ¼rÃ¼r.
        """
        inputs = {"query": query, "chat_history": chat_history}
        # GrafiÄŸi bu baÅŸlangÄ±Ã§ durumuyla Ã§alÄ±ÅŸtÄ±r
        final_state = self.graph.invoke(inputs)
        return final_state["response"]

def main():
    """Chatbot'u interaktif modda Ã§alÄ±ÅŸtÄ±rÄ±r."""

    chatbot = LangGraphChatbot()
    
    # HafÄ±za, ana dÃ¶ngÃ¼de tutulur
    chat_history = []
    max_history = chatbot.config.get('chat', {}).get('max_history', 5)

    print("\n--- Jotform AI Agent ---")
    print("Merhaba! Jotform hakkÄ±nda ne Ã¶ÄŸrenmek istersiniz? (Ã‡Ä±kmak iÃ§in 'exit' yazÄ±n)")
    
    while True:
        user_query = input("\nðŸ‘¤ You: ")
        if user_query.lower() in ['exit', 'quit', 'q']:
            print("Bye!")
            break
            
        # GrafiÄŸi mevcut geÃ§miÅŸle birlikte Ã§alÄ±ÅŸtÄ±r
        response = chatbot.invoke(user_query, chat_history)
        
        print(f"\nðŸ¤– Agent: {response}")
        
        # HafÄ±zayÄ± gÃ¼ncelle
        chat_history.append(HumanMessage(content=user_query))
        chat_history.append(AIMessage(content=response))
        
        # HafÄ±zanÄ±n Ã§ok uzamasÄ±nÄ± engelle
        if len(chat_history) > max_history * 2:
            chat_history = chat_history[-(max_history * 2):]

if __name__ == "__main__":
    main()